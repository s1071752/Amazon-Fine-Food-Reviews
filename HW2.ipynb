{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主題: Amazon Fine Food Reviews\n",
    "## 分類器: 隨機森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "\n",
    "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.      \n",
    "  \n",
    "\n",
    "    \n",
    "## HW2\n",
    ">- 本次作業為情緒分析，資料集為Amazon Fine Food Reviews 的Reviews.csv\n",
    ">- https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "\n",
    ">- HW2 Kaggle 競賽網址：\n",
    ">- https://www.kaggle.com/t/3d90c24a5d754706833a49b7842739a9\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 資料前處理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 讀取資料\n",
    "- 讀取csv檔前 10000 筆資料\n",
    "- 僅保留Text、Score兩個欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>1</td>\n",
       "      <td>we switched from the advance similac to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>5</td>\n",
       "      <td>Like the bad reviews say, the organic formula ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>5</td>\n",
       "      <td>I wanted to solely breastfeed but was unable t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>5</td>\n",
       "      <td>i love the fact that i can get this delieved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>4</td>\n",
       "      <td>We have a 7 week old... He had gas and constip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                               Text\n",
       "9995      1  we switched from the advance similac to the or...\n",
       "9996      5  Like the bad reviews say, the organic formula ...\n",
       "9997      5  I wanted to solely breastfeed but was unable t...\n",
       "9998      5  i love the fact that i can get this delieved t...\n",
       "9999      4  We have a 7 week old... He had gas and constip..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 主要使用 Reviews.csv這份資料\n",
    "data = pd.read_csv(\"Reviews.csv\", header=0, usecols=[\"Text\",\"Score\"], encoding='utf-8')[:10000]  #train\n",
    "testData = pd.read_csv(\"test.csv\", header=0, usecols=[\"Text\"], encoding='utf-8')         # test\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text欄位空值: 0\n",
      "Score欄位空值: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Text欄位空值:\", data['Text'].isnull().sum())\n",
    "print(\"Score欄位空值:\", data['Score'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 資料轉換\n",
    "- 將Score欄位內值大於等於4的轉成1(positive), 其餘轉成0 (negative)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      1  I have bought several of the Vitality canned d...\n",
       "1      0  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      1  This is a confection that has been around a fe...\n",
       "3      0  If you are looking for the secret ingredient i...\n",
       "4      1  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['Score']<4,'Score']=0\n",
    "data.loc[data['Score']>=4,'Score']=1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 文字前處理\n",
    "\n",
    "1. 去除標點符號 \n",
    "2. 統一大小寫\n",
    "3. sentence segmentation (斷句)\n",
    "4. word segmentation (斷詞)\n",
    "5. stopword (去除停用詞，例如i, with, and)\n",
    "6. pos 詞性標記\n",
    "7. Lemmatization辭型還原(避免將同樣的字詞例如love/loves/loved視為不同的輸入)\n",
    "\n",
    "\n",
    "參考文章: \n",
    "https://clay-atlas.com/blog/2019/07/30/nlp-python-cn-nltk-kit/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(tokens):\n",
    "    pos = [nltk.pos_tag(token) for token in tokens]  # 詞性標記 pos\n",
    "    wordnet_pos = []\n",
    "    \n",
    "    for p in pos:\n",
    "        for word, tag in p:\n",
    "            if tag.startswith('J'):\n",
    "                wordnet_pos.append(nltk.corpus.wordnet.ADJ)\n",
    "            elif tag.startswith('V'):\n",
    "                wordnet_pos.append(nltk.corpus.wordnet.VERB)\n",
    "            elif tag.startswith('N'):\n",
    "                wordnet_pos.append(nltk.corpus.wordnet.NOUN)\n",
    "            elif tag.startswith('R'):\n",
    "                wordnet_pos.append(nltk.corpus.wordnet.ADV)\n",
    "            else:\n",
    "                wordnet_pos.append(nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "# 辭型還原 Lemmatization\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(p[n][0], pos=wordnet_pos[n]) for p in pos for n in range(len(p))]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  #使用Nature Language Tool Kit (NLTK)進行文本處理\n",
    "import string\n",
    "\n",
    "####文本清理####\n",
    "def text_preprocessing(method, text):\n",
    "       \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  #刪去標點符號\n",
    "    text = text.lower() # 統一轉為小寫\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text) # 斷句 \n",
    "    tokens = [nltk.tokenize.word_tokenize(sent) for sent in sentences]  # 斷詞\n",
    "    \n",
    "    nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens[0] if token not in nltk_stopwords] # 僅保留非停用字(去除停用字)\n",
    "    \n",
    "    tokens_pos = pos([tokens])  #詞性標記\n",
    "    \n",
    "    if method =='tfidf': \n",
    "        text=\"\"        \n",
    "        for t in tokens_pos:\n",
    "            text += t\n",
    "            text += \" \"\n",
    "        return text  #需要回傳string\n",
    "   \n",
    "    elif method =='word2vec':\n",
    "        \n",
    "        return tokens_pos # 需要回傳list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4 文字轉向量\n",
    "訓練模型前要將文字轉為機器可閱讀的形式，以前使用的one-hot/dummy會造成高維稀疏的向量矩陣，因此採計算詞向量的方法，以下實作 tf-idf 及 word2vec 並進行比較\n",
    "\n",
    "#### (註: 1-4部分選擇其中一種執行即可，都執行只會有後寫入的word2vec結果)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf Vectorizer\n",
    ">- max_features 挑選出多少個有代表性的文字\n",
    ">- min_df/max_df 向量值高/低於此才會挑選，避免出現無代表性的詞彙\n",
    ">- 教學- https://ithelp.ithome.com.tw/articles/10228481\n",
    "\n",
    "變數 | 意義     \n",
    "----------------- | -----------------  \n",
    "text_cleaned| 去除標點的train data      \n",
    "test_text_cleaned| 去除標點的test data    \n",
    "mix_text|  以上兩者相加        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4-1 文本前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned = []  # 清乾淨的train data\n",
    "test_text_cleaned = []  # 清乾淨的test data\n",
    "\n",
    "\n",
    "for text in data['Text']:\n",
    "    \n",
    "    t = text_preprocessing('tfidf',text)\n",
    "    text_cleaned.append(t)\n",
    "   \n",
    "for text in testData['Text']:\n",
    "    \n",
    "    t = text_preprocessing('tfidf', text)\n",
    "    test_text_cleaned.append(t)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理前:  I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
      "清理後:  dont know cactus tequila unique combination ingredient flavour hot sauce make one kind pick bottle trip bring back home u totally blow away realize simply couldnt find anywhere city bummedbr br magic internet case sauce ecstatic itbr br love hot saucei mean really love hot sauce dont want sauce tastelessly burn throat grab bottle tequila picante gourmet de inclan realize taste never want use saucebr br thank personal incredible service \n"
     ]
    }
   ],
   "source": [
    "print(\"清理前: \", data['Text'][10])\n",
    "print(\"清理後: \", text_cleaned[10])\n",
    "                         #    i\n",
    "\n",
    "    \n",
    "mix_text = [] #將文字合併，後續一起計算向量\n",
    "mix_text.extend(text_cleaned)\n",
    "mix_text.extend(test_text_cleaned) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4-2 文字轉向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                                                       【tf-idf 】\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import csv \n",
    "\n",
    "\n",
    "def toVec(textArr): \n",
    "                                                                                      \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', token_pattern=\"(?u)\\\\b\\\\w+\\\\b\", max_features=500, min_df=0.0001, max_df=0.8)\n",
    "    tfidf_X = (vectorizer.fit_transform(textArr)) #text to vector\n",
    "    r = pd.DataFrame(tfidf_X.toarray(),columns=vectorizer.get_feature_names()) # show table\n",
    "     \n",
    "    return r  #vectors\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>2</th>\n",
       "      <th>20</th>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldnt</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1   10  100   12   15    2   20   24    3    4  ...  wont  work  \\\n",
       "14995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "14996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "14997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "14998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "14999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "       worth  wouldnt  wrong      year  yes  youll  youre  yummy  \n",
       "14995    0.0      0.0    0.0  0.000000  0.0    0.0    0.0    0.0  \n",
       "14996    0.0      0.0    0.0  0.205444  0.0    0.0    0.0    0.0  \n",
       "14997    0.0      0.0    0.0  0.000000  0.0    0.0    0.0    0.0  \n",
       "14998    0.0      0.0    0.0  0.000000  0.0    0.0    0.0    0.0  \n",
       "14999    0.0      0.0    0.0  0.287091  0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果是train, test分開算向量，最後選出的特徵不一致\n",
    "# 因此使用以下的方式，一併作訓練\n",
    "text_vector = toVec(mix_text)\n",
    "\n",
    "#將剛剛轉好的文字vector 繪製出table-->出現max_features個新欄位 (max_features=500)\n",
    "text_vector.tail()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4-3 新資訊併入原本的數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iebi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>2</th>\n",
       "      <th>20</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldnt</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.513584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text    1   10  100  \\\n",
       "0      1  I have bought several of the Vitality canned d...  0.0  0.0  0.0   \n",
       "1      0  Product arrived labeled as Jumbo Salted Peanut...  0.0  0.0  0.0   \n",
       "2      1  This is a confection that has been around a fe...  0.0  0.0  0.0   \n",
       "3      0  If you are looking for the secret ingredient i...  0.0  0.0  0.0   \n",
       "4      1  Great taffy at a great price.  There was a wid...  0.0  0.0  0.0   \n",
       "\n",
       "    12   15    2   20   24  ...  wont  work  worth  wouldnt  wrong  year  yes  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0    0.0      0.0    0.0   0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0    0.0      0.0    0.0   0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0    0.0      0.0    0.0   0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0    0.0      0.0    0.0   0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0    0.0      0.0    0.0   0.0  0.0   \n",
       "\n",
       "   youll  youre     yummy  \n",
       "0    0.0    0.0  0.000000  \n",
       "1    0.0    0.0  0.000000  \n",
       "2    0.0    0.0  0.301741  \n",
       "3    0.0    0.0  0.000000  \n",
       "4    0.0    0.0  0.513584  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector = text_vector[0:10000]\n",
    "data_ok = pd.concat([data, train_vector], axis=1, join='inner') \n",
    "\n",
    "test_vector = text_vector[10000:15000]\n",
    "test_vector.index = range(len(test_vector)) #要調整id才能正確join\n",
    "testData_ok = pd.concat([testData, test_vector], axis=1, join_axes=[testData.index])\n",
    "\n",
    "data_ok.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*使用TfidfVectorizer轉為向量完成*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    ">- size：特徵向量的維度，預設值為100。\n",
    ">- min_count: 在n篇文章之中，出現在少於min_count篇的單字會被丟掉，預設值是5。\n",
    ">- max_count: 出現頻率大於max_count的不納入，避免納入沒有辨識性的單字。\n",
    ">- 教學- https://blog.csdn.net/weixin_45599022/article/details/109008368\n",
    "\n",
    "變數 | 意義     \n",
    "----------------- | -----------------  \n",
    "train_cleaned| 去除標點的train data      \n",
    "test_cleaned| 去除標點的test data    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4-1文字前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_cleaned = []  # 清乾淨的train data\n",
    "test_cleaned = []  # 清乾淨的test data\n",
    "vocab = []  # 給模型的資料集\n",
    "\n",
    "for text in data['Text']:\n",
    "    \n",
    "    t = text_preprocessing('word2vec',text)\n",
    "    train_cleaned.append(t) \n",
    "    vocab.extend(t) # 只加入list內的元素而非整個[list]\n",
    "   \n",
    "for text in testData['Text']:\n",
    "    \n",
    "    t = text_preprocessing('word2vec', text)\n",
    "    test_cleaned.append(t)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理前:  I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
      "清理後:  ['dont', 'know', 'cactus', 'tequila', 'unique', 'combination', 'ingredient', 'flavour', 'hot', 'sauce', 'make', 'one', 'kind', 'pick', 'bottle', 'trip', 'bring', 'back', 'home', 'u', 'totally', 'blow', 'away', 'realize', 'simply', 'couldnt', 'find', 'anywhere', 'city', 'bummedbr', 'br', 'magic', 'internet', 'case', 'sauce', 'ecstatic', 'itbr', 'br', 'love', 'hot', 'saucei', 'mean', 'really', 'love', 'hot', 'sauce', 'dont', 'want', 'sauce', 'tastelessly', 'burn', 'throat', 'grab', 'bottle', 'tequila', 'picante', 'gourmet', 'de', 'inclan', 'realize', 'taste', 'never', 'want', 'use', 'saucebr', 'br', 'thank', 'personal', 'incredible', 'service']\n"
     ]
    }
   ],
   "source": [
    "print(\"清理前: \", data['Text'][10])\n",
    "print(\"清理後: \", train_cleaned[10])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4-2 文字轉向量\n",
    "word2vec: https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\n",
    "\n",
    "教學: https://www.kaggle.com/code/jerrykuo7727/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\") #生成都是0.的向量矩陣featureVec\n",
    "    nwords = 0.\n",
    "\n",
    "    # Index2word中包含了詞表中的所有詞，为了檢索速度，保存到set中\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.                               \n",
    "            featureVec = np.add(featureVec, model.wv[word]) # 如果評論中的詞有出現在詞表中,用model.wv[]把字典中那個字的詞向量提取出來，加到featureVec\n",
    "        \n",
    "    # 將featureVec取平均\n",
    "    if nwords != 0.:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6258463e-03,  4.1048890e-03, -1.5675224e-01,  2.1065138e-01,\n",
       "       -6.5126158e-02, -5.6438905e-01,  8.1337148e-01,  1.3938509e+00,\n",
       "       -1.2258177e+00, -2.1372755e-01, -4.9288031e-02, -6.8298507e-01,\n",
       "        1.6987462e-01,  2.4053688e-01, -4.6101040e-01,  3.9932135e-01,\n",
       "        8.6640102e-01,  5.3971380e-01, -1.0271988e+00, -2.8547239e-01,\n",
       "        3.4018785e-01,  7.3453510e-01,  1.4959332e+00, -3.8316351e-01,\n",
       "        1.8808918e-01,  4.6152154e-01, -6.7901200e-01,  4.5624051e-02,\n",
       "       -4.9062592e-01,  4.5379862e-01,  6.9938105e-04, -3.0936080e-01,\n",
       "       -2.2072946e-01, -6.2258035e-01, -2.2877641e-01,  1.5007618e-01,\n",
       "        5.0723785e-01,  1.9628076e-01,  3.0430606e-01,  7.8947805e-02,\n",
       "        1.1283092e+00, -4.8560354e-01, -5.0631058e-01, -1.2747544e-01,\n",
       "        9.4562042e-01,  2.7307609e-01, -5.1591897e-01, -6.0097528e-01,\n",
       "        6.9958103e-01, -8.9159511e-02], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv['like']  \n",
    "# 若上面那段code的num_features=5  --> featureVec長度為5，一開始內容都是0.--> [0.0.0.0.0.]\n",
    "# 若句子中有like單字，like的向量為[-1.625,  4.104, -1.567,  2.106, -6.5126]，featureVec會各自加上這5個向量\n",
    "# 重複此步驟直到句子中單字都檢索完畢\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts  #使用common_texts訓練用的詞彙\n",
    "\n",
    "model = Word2Vec([vocab], min_count=1, vector_size=50) # vector_size詞向量的維度大小(預設100)\n",
    "#model.build_vocab(vocab)  # prepare the model vocabulary\n",
    "model.train([vocab], total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
    "\n",
    "vec = []\n",
    "\n",
    "def word2vec(text):\n",
    "    for sentence in text:\n",
    "        vec.append(makeFeatureVec(sentence, model, 50))  # 對每一份評論中的所有詞向量取平均      \n",
    "         # num_features 要跟word2vec model的參數設定相同(50)\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4-3 向量併入原本的數據集(或是應該取平均試試?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>0.179682</td>\n",
       "      <td>0.053117</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>-0.086589</td>\n",
       "      <td>-0.196434</td>\n",
       "      <td>-0.146175</td>\n",
       "      <td>0.284249</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304125</td>\n",
       "      <td>-0.037870</td>\n",
       "      <td>-0.120040</td>\n",
       "      <td>-0.080275</td>\n",
       "      <td>0.467188</td>\n",
       "      <td>-0.111327</td>\n",
       "      <td>-0.005847</td>\n",
       "      <td>-0.055848</td>\n",
       "      <td>0.103837</td>\n",
       "      <td>0.059922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0.078641</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>-0.042555</td>\n",
       "      <td>-0.093654</td>\n",
       "      <td>-0.066991</td>\n",
       "      <td>0.131246</td>\n",
       "      <td>0.180712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140608</td>\n",
       "      <td>-0.012672</td>\n",
       "      <td>-0.058863</td>\n",
       "      <td>-0.037818</td>\n",
       "      <td>0.215009</td>\n",
       "      <td>-0.059507</td>\n",
       "      <td>-0.001136</td>\n",
       "      <td>-0.024000</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>0.030461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>0.050705</td>\n",
       "      <td>0.012896</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>-0.023607</td>\n",
       "      <td>-0.048641</td>\n",
       "      <td>-0.038698</td>\n",
       "      <td>0.071485</td>\n",
       "      <td>0.097072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073911</td>\n",
       "      <td>-0.014736</td>\n",
       "      <td>-0.027373</td>\n",
       "      <td>-0.018616</td>\n",
       "      <td>0.115827</td>\n",
       "      <td>-0.030276</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.016058</td>\n",
       "      <td>0.028103</td>\n",
       "      <td>0.015771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>0.034584</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>-0.062333</td>\n",
       "      <td>-0.126994</td>\n",
       "      <td>-0.094435</td>\n",
       "      <td>0.182518</td>\n",
       "      <td>0.247832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197551</td>\n",
       "      <td>-0.020734</td>\n",
       "      <td>-0.072768</td>\n",
       "      <td>-0.056587</td>\n",
       "      <td>0.298782</td>\n",
       "      <td>-0.069288</td>\n",
       "      <td>-0.000492</td>\n",
       "      <td>-0.043348</td>\n",
       "      <td>0.062178</td>\n",
       "      <td>0.040268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>-0.042790</td>\n",
       "      <td>-0.100424</td>\n",
       "      <td>-0.073969</td>\n",
       "      <td>0.142099</td>\n",
       "      <td>0.183185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148942</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.061330</td>\n",
       "      <td>-0.042799</td>\n",
       "      <td>0.226928</td>\n",
       "      <td>-0.044780</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>-0.035604</td>\n",
       "      <td>0.049735</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text         0  \\\n",
       "0      1  I have bought several of the Vitality canned d...  0.179682   \n",
       "1      0  Product arrived labeled as Jumbo Salted Peanut...  0.078641   \n",
       "2      1  This is a confection that has been around a fe...  0.050705   \n",
       "3      0  If you are looking for the secret ingredient i...  0.119093   \n",
       "4      1  Great taffy at a great price.  There was a wid...  0.085194   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.053117  0.006822 -0.086589 -0.196434 -0.146175  0.284249  0.388268  ...   \n",
       "1  0.024564  0.002057 -0.042555 -0.093654 -0.066991  0.131246  0.180712  ...   \n",
       "2  0.012896  0.002131 -0.023607 -0.048641 -0.038698  0.071485  0.097072  ...   \n",
       "3  0.034584  0.004795 -0.062333 -0.126994 -0.094435  0.182518  0.247832  ...   \n",
       "4  0.027555  0.000479 -0.042790 -0.100424 -0.073969  0.142099  0.183185  ...   \n",
       "\n",
       "         40        41        42        43        44        45        46  \\\n",
       "0  0.304125 -0.037870 -0.120040 -0.080275  0.467188 -0.111327 -0.005847   \n",
       "1  0.140608 -0.012672 -0.058863 -0.037818  0.215009 -0.059507 -0.001136   \n",
       "2  0.073911 -0.014736 -0.027373 -0.018616  0.115827 -0.030276 -0.000257   \n",
       "3  0.197551 -0.020734 -0.072768 -0.056587  0.298782 -0.069288 -0.000492   \n",
       "4  0.148942 -0.016424 -0.061330 -0.042799  0.226928 -0.044780 -0.003636   \n",
       "\n",
       "         47        48        49  \n",
       "0 -0.055848  0.103837  0.059922  \n",
       "1 -0.024000  0.048307  0.030461  \n",
       "2 -0.016058  0.028103  0.015771  \n",
       "3 -0.043348  0.062178  0.040268  \n",
       "4 -0.035604  0.049735  0.026044  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_vector = pd.DataFrame(word2vec(train_cleaned))\n",
    "data_ok = pd.concat([data, train_vector], axis=1, join='inner') \n",
    "\n",
    "test_vector =  pd.DataFrame(word2vec(test_cleaned))\n",
    "test_vector.index = range(len(test_vector)) #要調整id才能正確join\n",
    "testData_ok = pd.concat([testData, test_vector], axis=1, join='inner')\n",
    "\n",
    "data_ok.head()\n",
    "# 因為num_features設為50，故增加50列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*使用Word2vec轉為向量完成*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 建立模型( Random forest )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 輸入的資料型態為 pd.DataFrame(文字的部分要轉為向量)\n",
    ">- n_estimators : 隨機森林中的樹數量，我覺得設定高一些效果較好，相對地挑選代表詞彙時要挑選更多個詞。  \n",
    ">- min_samples_split : 劃分出新分支點所需要的最小樣本數\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "                                              ####  【輸出預測結果，上傳至kaggle】  ####\n",
    "\n",
    "def predict_testData(k, model):\n",
    "    \n",
    "    predicted = model.predict(testData_ok.drop(labels=['Text'], axis=1))\n",
    "# 讀取test.csv，輸出結果至test.csv\n",
    "    id = [i for i in range(1, 5000+1)]\n",
    "    results = {\n",
    "            'ID': id,   #輸出共有兩行，第一行是角色編號，第二行是(預測結果)是否死亡\n",
    "            'Score': predicted\n",
    "    }\n",
    "\n",
    "    submission = pd.DataFrame(results)\n",
    "\n",
    "    submission.to_csv(str(k)+\"-submission.csv\", index=False, header=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                                                ####  【進行k-fold cross-validation】  #### \n",
    "\n",
    "def k_fold(k, data):  #.copy()\n",
    "    \n",
    "    folds = np.array_split(data, k) #將data切成k份，其中1份當測試集，剩餘k-1份當訓練集建立模型\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(k): # cross validation\n",
    "\n",
    "        train_data = folds.copy()  \n",
    "        del train_data[i]       \n",
    "        train_data = pd.concat(train_data, sort=False)\n",
    "        \n",
    "        test_data = folds[i]\n",
    "\n",
    "        x_train = train_data.drop(labels=['Score','Text'], axis=1)  #id--> axis 0   attribute-->axis 1 \n",
    "        y_train = train_data['Score']\n",
    "        x_test = test_data.drop(labels=['Score','Text'], axis=1) # testdata\n",
    "        y_test = test_data['Score']                              # testdata label\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=300, min_samples_split=2)\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        accuracy += clf.score(x_test, y_test)\n",
    "    \n",
    "        predict_testData(k, clf)\n",
    "        \n",
    "    return accuracy/k  \n",
    "\n",
    "#輪流將k份的每份資料都當 測試集，其餘當訓練集建立模型，因此會進行k次，k次都計算出Accuracy\n",
    "#將k次的Accuracy平均即為output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. tf-idf 、 word2vec 之結果比較\n",
    "\n",
    "| 方法 | 計算方式 | 結果比較     \n",
    "| :----: | :--- | :--- \n",
    "| tf-idf | 根據詞彙在整份文章的出現頻率以及在其它文本的出現頻率，判斷詞彙的重要性將文字轉換成向量 | 可以計算字詞在主題上的代表性，像是在此範例中有提取出glad, prefer, yummy等詞，我覺得對此次測試資料的預測較合適    \n",
    "| word2vec | 將文中詞彙計算cosin值，cosin值相近的詞彙意義較接近 | 比較不參考文字在文本中的重要性，若將詞彙向量取平均作為文本的平均向量在此次預測的效果會不好，沒有明顯看出正向評價與負向評價的向量值差異，應該針對其中的關鍵詞作加權    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8147"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold(4, data_ok) #### (註: 1-4部分選擇其中一種執行即可，都執行只會有後寫入的word2vec結果)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612999999999999"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold(4, data_ok)### (註: 1-4部分選擇其中一種執行即可，都執行只會有後寫入的word2vec結果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
